{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4f9f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(\"--- 环境诊断开始 ---\")\n",
    "\n",
    "# 1. 检查 PyTorch 版本\n",
    "print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "\n",
    "# 2. 检查 Transformers 版本\n",
    "print(f\"Transformers 版本: {transformers.__version__}\")\n",
    "\n",
    "# 3. 核心测试：检查GPU是否可用\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "print(f\"\\nGPU 是否可用: {is_cuda_available}\")\n",
    "\n",
    "if is_cuda_available:\n",
    "    # 如果GPU可用，打印更多信息\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    current_gpu_index = torch.cuda.current_device()\n",
    "    gpu_name = torch.cuda.get_device_name(current_gpu_index)\n",
    "    \n",
    "    print(f\"检测到 {gpu_count} 个GPU。\")\n",
    "    print(f\"当前使用的GPU索引: {current_gpu_index}\")\n",
    "    print(f\"当前GPU名称: {gpu_name}\")\n",
    "    \n",
    "    # 简单的张量运算测试，确认GPU可以工作\n",
    "    try:\n",
    "        x = torch.tensor([1.0, 2.0, 3.0]).to(\"cuda\")\n",
    "        print(f\"成功将张量移动到GPU: {x}\")\n",
    "        print(\"GPU工作正常！\")\n",
    "    except Exception as e:\n",
    "        print(f\"GPU运算时出错: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"未检测到可用的NVIDIA GPU。模型将会在CPU上运行。\")\n",
    "    print(\"（注意：在CPU上运行大模型会非常慢。）\")\n",
    "\n",
    "print(\"\\n--- 环境诊断结束 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5196cc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# --- 1. 定义模型ID和加载配置 ---\n",
    "# 我们选择Qwen1.5的7B对话模型\n",
    "model_id = \"Qwen/Qwen1.5-7B-Chat\"\n",
    "\n",
    "print(f\"正在加载模型: {model_id}\")\n",
    "print(\"这将在第一次运行时下载模型文件（约15GB），请耐心等待...\")\n",
    "\n",
    "# 创建文本生成管道\n",
    "# device_map=\"auto\" 会自动将模型加载到您的RTX 4060 Ti上\n",
    "# torch_dtype=\"auto\" 会为您的显卡自动选择最佳的数据类型（bfloat16）\n",
    "# load_in_4bit=True 会启用4-bit量化，极大降低显存占用\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    model_kwargs={\"load_in_4bit\": True}\n",
    ")\n",
    "\n",
    "print(\"模型加载完毕，准备生成文本！\")\n",
    "\n",
    "# --- 2. 创建对话内容 ---\n",
    "# 对于对话模型，我们需要遵循它指定的对话格式\n",
    "# 这里我们创建一个包含系统指令和用户问题的消息列表\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"你好！请你用中文介绍一下自己，并写一首关于夏天的五言绝句。\"}\n",
    "]\n",
    "\n",
    "# 使用分词器（tokenizer）的应用聊天模板功能，将消息列表转换为模型能理解的单个字符串\n",
    "# 这是与对话模型交互的推荐做法\n",
    "prompt = pipe.tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "\n",
    "# --- 3. 生成并打印回复 ---\n",
    "# do_sample=True 允许使用更多样化的生成策略\n",
    "# temperature 和 top_p 用于控制生成文本的创造性和随机性\n",
    "# max_new_tokens 控制模型最多生成多少个新词\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95\n",
    ")\n",
    "\n",
    "# outputs[0][\"generated_text\"] 包含了完整的对话（包括我们的输入）\n",
    "# 我们只关心模型生成的部分，所以需要切片掉输入的部分\n",
    "response = outputs[0][\"generated_text\"][len(prompt):]\n",
    "\n",
    "print(\"\\n--- AI 的回复 ---\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4666c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5763590e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.7481212615966797},\n",
       " {'label': 'POSITIVE', 'score': 0.9943673014640808},\n",
       " {'label': 'POSITIVE', 'score': 0.9821881651878357},\n",
       " {'label': 'POSITIVE', 'score': 0.9924151301383972},\n",
       " {'label': 'NEGATIVE', 'score': 0.9922665953636169}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier([\"\",\"123\",\"1234\",\"china\",\"Iraq\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5688337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
